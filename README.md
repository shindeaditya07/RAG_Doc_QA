
# RAG Document Q&A Application Using Groq and OpenAI

This project allows you to upload documents, build vector indexes, and perform semantic search using embeddings.

---

## ðŸš€ **Project Overview**

The system uses the following workflow:

1. **Upload a Document** â†’ File is saved to a temporary folder and stored in session state.
2. **Build Index** â†’ The uploaded document is split into smaller chunks and indexed into a vector database using embeddings.
3. **Query** â†’ The vector database can then be used to answer queries or perform semantic searches.
4. **Chat History** â†’ All user questions and assistant answers are saved and displayed, so you can view the full conversation flow while interacting with your document.

---

## ðŸ“š **Libraries and Their Uses**

- **streamlit** â†’ Interactive web application where users can upload PDFs, enter queries, and view results in real-time.
- **langchain** â†’ Framework for creating the conversational pipeline (document loaders, text splitters, embeddings, retrieval, and LLM chains).
- **openai** â†’ Provides access to OpenAIâ€™s embedding models (`text-embedding-ada-002`) and LLMs for generating answers to user queries.
- **faiss-cpu** â†’ Vector database for storing and efficiently searching embeddings of PDF chunks, enabling fast similarity search.
- **python-dotenv** â†’ Loads API keys (OpenAI, Groq, etc.) from a `.env` file securely, keeping credentials out of the code.
- **langchain-community** â†’ Additional community-supported integrations like document loaders and utility tools.
- **langchain-openai** â†’ LangChain wrappers for OpenAIâ€™s models (embeddings + chat models) to integrate into the pipeline seamlessly.
- **langchain-groq** â†’ Integrates Groqâ€™s LLMs into the chatbot as an alternative to OpenAI.
- **pypdf** â†’ Lightweight library to read and parse text from PDF documents.
- **pymupdf** â†’ Advanced PDF parser that extracts text, metadata, and images.
- **langchain-text-splitters** â†’ Splits large chunks of PDF text into smaller, manageable segments for embedding and searching.

---

## ðŸ“‚ **File Upload Process**

When you upload a file in **Streamlit**, the system:

- Stores it locally.
- Keeps track of it in `st.session_state.docs_info`.

**Key Attributes:**

- `uploaded_file` â†’ The file object uploaded through Streamlit.
- `file_path` â†’ The path where the uploaded file is stored locally.
- `st.session_state.docs_info` â†’ A list storing metadata of uploaded files, including their name and path.

---

## ðŸ›  **Building the Index**

When you click **Build Index**, the system takes the last uploaded document and:

1. Loads it using `load_documents(file_path)`.
2. Splits it into smaller overlapping chunks using `chunk_documents(...)`.
3. Creates embeddings using `build_embeddings_model(...)`.
4. Stores the chunks into a vector database using `create_vector_store(...)` for semantic search.

---

## ðŸ’¬ **Chat History Feature**

The app includes a **chat interface** where both user queries and assistant responses are displayed in sequence:

- Each user input is tagged as **User**, and the modelâ€™s response as **Assistant**.
- History persists throughout the session, allowing you to scroll back and review earlier questions and answers.
- Provides a **continuous conversational way** of exploring your documents.

---

## ðŸ“„ **Example of `docs_info`**

If you upload two files:

- `report.pdf` â†’ stored at `uploaded_docs/report.pdf`
- `notes.txt` â†’ stored at `uploaded_docs/notes.txt`

When you click **Build Index**, the system will use the **last uploaded file (`notes.txt`)** for indexing.

---

## âœ… **Summary**

- **file_path** â†’ Local reference to the uploaded document stored inside `uploaded_docs/`.
- **st.session_state.docs_info** â†’ Tracks documents with file names and stored locations.
- **Chat History** â†’ Stores all queries and answers, ensuring a smooth conversational experience.

---
